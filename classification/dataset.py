import torch
import re
import glob
from torch.utils.data import Dataset
import os
import numpy as np
from PIL import Image
from torchvision import transforms


# ============================================================
# ðŸ§© 1ï¸âƒ£ HÃ m Ä‘á»c label tá»« tÃªn file [xxxxx]
# ============================================================
def get_file_label(filename: str, num_class: int = 5) -> np.ndarray:
    """
    TrÃ­ch one-hot label tá»« tÃªn file BCSS-WSSS, vÃ­ dá»¥:
    - 'patch_001_[10000].png'  â†’ [1,0,0,0,0]
    - 'patch_057_[00101].png'  â†’ [0,0,1,0,1]
    """
    term_split = re.split(r"\[|\]", filename)
    if len(term_split) < 2:
        raise ValueError(f"TÃªn file khÃ´ng chá»©a nhÃ£n trong dáº¥u []: {filename}")
    label_str = term_split[1]
    cls_label = np.array([int(x) for x in label_str[:num_class]], dtype=np.int64)
    return cls_label


# ============================================================
# ðŸ§  2ï¸âƒ£ Dataset cho huáº¥n luyá»‡n (áº£nh + label tá»« filename)
# ============================================================
class TrainPatchesDataset(Dataset):
    """
    Dataset dÃ¹ng cho giai Ä‘oáº¡n classification training (Stage 1).
    - áº¢nh láº¥y tá»« folder `train`
    - Label láº¥y tá»« tÃªn file (one-hot [xxxxx])
    """
    def __init__(self, data_path, transform=None, num_class=5):
        self.path = data_path
        self.files = sorted([
            f for f in os.listdir(data_path)
            if not f.startswith('.') and f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ])
        self.transform = transform
        self.num_class = num_class

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        fname = self.files[idx]
        img_path = os.path.join(self.path, fname)
        img = Image.open(img_path).convert("RGB")
        label = get_file_label(fname, num_class=self.num_class)

        if self.transform:
            img = self.transform(img)
        else:
            img = transforms.ToTensor()(img)

        return img, torch.tensor(label, dtype=torch.float32)


# ============================================================
# ðŸ§© 3ï¸âƒ£ Dataset cho validation (áº£nh + mask thá»±c táº¿)
# ============================================================
class ValidImageMaskDataset(Dataset):
    """
    Dataset dÃ¹ng cho validation segmentation.
    - áº¢nh Ä‘á»c tá»« `valid/img`
    - Mask Ä‘á»c tá»« `valid/mask`
    - Validation dataset: cÃ³ image + mask.DÃ¹ng Ä‘á»ƒ sinh CAM vÃ  tÃ­nh IoU.
    """
    def __init__(self, img_dir, mask_dir, transform=None):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.img_files = sorted([
            f for f in os.listdir(img_dir)
            if not f.startswith('.') and f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ])
        self.transform = transform

    def __len__(self):
        return len(self.img_files)

    def __getitem__(self, idx):
        fname = self.img_files[idx]
        img_path = os.path.join(self.img_dir, fname)
        mask_path = os.path.join(self.mask_dir, fname)

        img = Image.open(img_path).convert("RGB")
        mask = Image.open(mask_path).convert("L")  # mask grayscale (0â€“Câˆ’1)
        if self.transform:
            img = self.transform(img)
        else:
            img = transforms.ToTensor()(img)

        mask = torch.tensor(np.array(mask), dtype=torch.long)

        return img, mask, fname

# ============================================================
# ðŸ”¬ 4ï¸âƒ£ Dataset dÃ¹ng cho multi-scale CAM (Stage 2)
# ============================================================
class TrainingSetCAM(Dataset):
    """
    Dataset sinh patch Ä‘a tá»‰ lá»‡ Ä‘á»ƒ train segmentation vá»›i CAM.
    - Sá»­ dá»¥ng multiscale_online_crop() Ä‘á»ƒ táº¡o patch táº¡i nhiá»u scale
    """
    def __init__(self, data_path, transform, patch_size, stride, scales, num_class=5):
        self.path = data_path
        self.files = sorted([
            f for f in os.listdir(data_path)
            if not f.startswith('.') and f.lower().endswith(('.png', '.jpg', '.jpeg'))
        ])
        self.transform = transform
        self.patch_size = patch_size
        self.stride = stride
        self.scales = scales
        self.num_class = num_class

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        fname = self.files[idx]
        img_path = os.path.join(self.path, fname)
        img = np.asarray(Image.open(img_path).convert("RGB"))
        scaled_im_list, scaled_pos_list = multiscale_online_crop(img, self.patch_size, self.stride, self.scales)

        if self.transform:
            for im_list in scaled_im_list:
                for i in range(len(im_list)):
                    im_list[i] = self.transform(im_list[i])

        label = get_file_label(fname, num_class=self.num_class)
        return fname, scaled_im_list, scaled_pos_list, self.scales, label

class PseudoSegDataset(Dataset):
    def __init__(self, img_dir, mask_dir, transform=None):
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.images = sorted(os.listdir(img_dir))
        self.masks = sorted(os.listdir(mask_dir))
        self.transform = transform

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        img = Image.open(os.path.join(self.img_dir, self.images[idx])).convert('RGB')
        mask = Image.open(os.path.join(self.mask_dir, self.masks[idx]))
        mask = np.array(mask, dtype=np.int64)

        if self.transform:
            img = self.transform(img)
        mask = torch.tensor(mask, dtype=torch.long)
        return img, mask
